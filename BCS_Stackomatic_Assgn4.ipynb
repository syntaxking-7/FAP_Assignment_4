{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fVZyymGbzZeW",
        "outputId": "69298d92-72c6-4464-ded1-dd5ad8928839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Collecting tetris_gymnasium\n",
            "  Downloading tetris_gymnasium-0.2.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from tetris_gymnasium) (1.0.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from tetris_gymnasium) (1.26.4)\n",
            "Requirement already satisfied: opencv-python<5.0.0.0,>=4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from tetris_gymnasium) (4.10.0.84)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.1->tetris_gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.1->tetris_gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.1->tetris_gymnasium) (0.0.4)\n",
            "Downloading tetris_gymnasium-0.2.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: tetris_gymnasium\n",
            "Successfully installed tetris_gymnasium-0.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install tetris_gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHZ_tLymyYW_"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from tetris_gymnasium.envs import Tetris\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMoiL2SU8TKf"
      },
      "source": [
        "# Set up Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WU__Xmn8aFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b67d6e74-3075-4919-af63-1ad45c76346b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x7b1dc960f210>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "env = gym.make('tetris_gymnasium/Tetris', render_mode='rgb_array')\n",
        "\n",
        "if 'inline' in matplotlib.get_backend():\n",
        "  from IPython import display\n",
        "plt.ion()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up for Training"
      ],
      "metadata": {
        "id": "8MQZWitRpl4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.fc1_1 = nn.Linear(24 * 18, 512)\n",
        "        self.fc1_2 = nn.Linear(24 * 18, 512)\n",
        "        self.fc1_3 = nn.Linear(4 * 4, 64)\n",
        "        self.fc1_4 = nn.Linear(4 * 16, 64)\n",
        "\n",
        "        self.fc2 = nn.Linear(512 + 512 + 64 + 64, 512)\n",
        "        self.fc3 = nn.Linear(512, 8)\n",
        "\n",
        "    def forward(self, board, active_tetromino_mask, holder, queue):\n",
        "        x1 = torch.relu(self.fc1_1(board.view(1, 432)))\n",
        "        x2 = torch.relu(self.fc1_2(active_tetromino_mask.view(1, 432)))\n",
        "        x3 = torch.relu(self.fc1_3(holder.view(1, 16)))\n",
        "        x4 = torch.relu(self.fc1_4(queue.view(1, 64)))\n",
        "\n",
        "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, action_size, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-4, batch_size=32, memory_size=10000):\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.model = DQN()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "        self.memory = ReplayMemory(memory_size)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "        board = torch.tensor(state['board'], dtype=torch.float32)\n",
        "        active_tetromino_mask = torch.tensor(state['active_tetromino_mask'], dtype=torch.float32)\n",
        "        holder = torch.tensor(state['holder'], dtype=torch.float32)\n",
        "        queue = torch.tensor(state['queue'], dtype=torch.float32)\n",
        "\n",
        "        q_values = self.model(board, active_tetromino_mask, holder, queue)\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = self.memory.sample(self.batch_size)\n",
        "        for state, action, reward, next_state, done in batch:\n",
        "            board, active_tetromino_mask, holder, queue = state.values()\n",
        "            next_board, next_active_tetromino_mask, next_holder, next_queue = next_state.values()\n",
        "\n",
        "            board_tensor = torch.tensor(board, dtype=torch.float32)\n",
        "            active_tetromino_mask_tensor = torch.tensor(active_tetromino_mask, dtype=torch.float32)\n",
        "            holder_tensor = torch.tensor(holder, dtype=torch.float32)\n",
        "            queue_tensor = torch.tensor(queue, dtype=torch.float32)\n",
        "\n",
        "            next_board_tensor = torch.tensor(next_board, dtype=torch.float32)\n",
        "            next_active_tetromino_mask_tensor = torch.tensor(next_active_tetromino_mask, dtype=torch.float32)\n",
        "            next_holder_tensor = torch.tensor(next_holder, dtype=torch.float32)\n",
        "            next_queue_tensor = torch.tensor(next_queue, dtype=torch.float32)\n",
        "\n",
        "            next_q_values = self.model(next_board_tensor, next_active_tetromino_mask_tensor, next_holder_tensor, next_queue_tensor)\n",
        "            next_q_value = torch.max(next_q_values).item()\n",
        "\n",
        "            target_q_value = reward + (1 - done) * self.gamma * next_q_value\n",
        "\n",
        "            current_q_values = self.model(board_tensor, active_tetromino_mask_tensor, holder_tensor, queue_tensor)\n",
        "            current_q_value = current_q_values[0][action]\n",
        "\n",
        "            loss = self.loss_fn(current_q_value, torch.tensor(target_q_value, dtype=torch.float32))\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def train(self, env, num_episodes=1000):\n",
        "        for e in range(num_episodes):\n",
        "            state, _ = env.reset()\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "\n",
        "            while not done:\n",
        "                action = self.act(state)\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                self.memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "                self.replay()\n",
        "\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "            print(f\"Episode {e}/{num_episodes}, Total Reward: {total_reward}, Epsilon: {self.epsilon}\")"
      ],
      "metadata": {
        "id": "b883-4w1cOxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "L78GK5_Qpt2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQNAgent(action_size=8)\n",
        "agent.train(env, num_episodes=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "collapsed": true,
        "id": "MaLp99VGcVl0",
        "outputId": "143ad20f-e92c-44de-f329-5c1c9e8fc576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/1000, Total Reward: 8, Epsilon: 0.9091562615825302\n",
            "Episode 1/1000, Total Reward: 9, Epsilon: 0.798065677681905\n",
            "Episode 2/1000, Total Reward: 10, Epsilon: 0.6866430931872001\n",
            "Episode 3/1000, Total Reward: 10, Epsilon: 0.5848838636585911\n",
            "Episode 4/1000, Total Reward: 9, Epsilon: 0.5344229416520513\n",
            "Episode 5/1000, Total Reward: 9, Epsilon: 0.4982051627146237\n",
            "Episode 6/1000, Total Reward: 10, Epsilon: 0.457510005540005\n",
            "Episode 7/1000, Total Reward: 10, Epsilon: 0.41386834584198684\n",
            "Episode 8/1000, Total Reward: 12, Epsilon: 0.322118930542046\n",
            "Episode 9/1000, Total Reward: 9, Epsilon: 0.2943280830920294\n",
            "Episode 10/1000, Total Reward: 9, Epsilon: 0.25966219297210513\n",
            "Episode 11/1000, Total Reward: 12, Epsilon: 0.18373897616330553\n",
            "Episode 12/1000, Total Reward: 13, Epsilon: 0.08619587539414532\n",
            "Episode 13/1000, Total Reward: 11, Epsilon: 0.04251487140556204\n",
            "Episode 14/1000, Total Reward: 13, Epsilon: 0.020044857891939702\n",
            "Episode 15/1000, Total Reward: 10, Epsilon: 0.011094979641301777\n",
            "Episode 16/1000, Total Reward: 16, Epsilon: 0.00998645168764533\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1040c56dfd21>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-9ea4b578d8ed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, num_episodes)\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-9ea4b578d8ed>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "ygJsLINgpwyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "done = False\n",
        "total_reward = 0\n",
        "frames = []\n",
        "state, _ = env.reset()\n",
        "\n",
        "while not done:\n",
        "  action = agent.act(state)\n",
        "  next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "  agent.memory.push(state, action, reward, next_state, done)\n",
        "  agent.replay()\n",
        "  frame = env.render()\n",
        "  frames.append(frame)\n",
        "  done = terminated or truncated\n",
        "  total_reward+=reward\n",
        "  state = next_state\n",
        "\n",
        "env.close()\n",
        "print(f'Total rewards: {total_reward}')\n",
        "\n",
        "import imageio\n",
        "video_path = 'tetris_test.mp4'\n",
        "imageio.mimsave(video_path, frames, fps=30)\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open(video_path,'rb').read()\n",
        "video_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f'<video width=\"640\" height=\"480\" controls><source src=\"{video_url}\" type=\"video/mp4\"></video>')"
      ],
      "metadata": {
        "id": "JfZZmwYzkKsv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}